{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv4zjcDkyiTD"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBfJS7wlxRDc"
      },
      "outputs": [],
      "source": [
        "# virtualenv venv -p /usr/bin/python3.7\n",
        "# source venv/bin/activate\n",
        "# pip3 install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip3 install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRVMNTtBwqlL"
      },
      "outputs": [],
      "source": [
        "!mkdir \"data\"\n",
        "%cd \"data\"\n",
        "!wget \"https://aristo-data-public.s3.amazonaws.com/proofwriter/proofwriter-dataset-V2020.12.3.zip\"\n",
        "# wget \"https://drive.google.com/uc?export=download&id=1kVr-YsUVFisceiIklvpWEe0kHNSIFtNh\"\n",
        "# mv \"uc?export=download&id=1kVr-YsUVFisceiIklvpWEe0kHNSIFtNh\" \"entailment_trees_emnlp2021_data_v3.zip\"\n",
        "!unzip \"proofwriter-dataset-V2020.12.3.zip\"\n",
        "# unzip \"entailment_trees_emnlp2021_data_v3.zip\"\n",
        "%cd \"..\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip3 install datasets numpy gsutil\n",
        "!gsutil cp -r gs://ai2-oyvindt/t5-models/hf-conversions/rr_owa_d3plus_infstage_ma1_mixture_large_hf ."
      ],
      "metadata": {
        "id": "49h4sLdpgF25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "uHTqLUzjEN9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "ve_wjPmJf04H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pVcR2rqpxn4F"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "import torch\n",
        "import spacy\n",
        "import numpy as np\n",
        "import transformers\n",
        "import joblib\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from torch.nn import Module\n",
        "from sklearn import tree, metrics, model_selection\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "TVFs9VBphGK0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_Oq9HBIyQvH"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N4m_NQJgyP52"
      },
      "outputs": [],
      "source": [
        "utils_since = time.time()\n",
        "\n",
        "def clear_cache():\n",
        "    gc.collect()\n",
        "    with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "    assert 1 / 0 == 0\n",
        "\n",
        "def set_device(gpu):\n",
        "    if gpu and torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    return device\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    return seed\n",
        "\n",
        "def my_print(header=None, text=None, mirror=None, timestamp=True, reset=False):\n",
        "    if reset:\n",
        "        global utils_since\n",
        "        utils_since = time.time()\n",
        "        if mirror is not None:\n",
        "            with open(mirror, 'wt') as f:\n",
        "                f.truncate(0)\n",
        "    result = \"\"\n",
        "    if header is not None:\n",
        "        result += f\"{header} -> \"\n",
        "    if text is not None:\n",
        "        result += text\n",
        "    if timestamp:\n",
        "        if text is not None:\n",
        "            result += \" | \"\n",
        "        now = datetime.timedelta(seconds=round(time.time()-utils_since))\n",
        "        result += f\"elapsed: {now}\"\n",
        "    print(result)\n",
        "    if mirror is not None:\n",
        "        with open(mirror, 'at') as f:\n",
        "            print(result, file=f)\n",
        "\n",
        "def save_model(model, tokenizer, optimizer, label, metadata):\n",
        "    path = os.path.join(metadata['SAVE_PATH'], f\"{metadata['MODEL_NAME']}_{label}\")\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "    torch.save(optimizer.state_dict(), os.path.join(path, 'optim.pt'))\n",
        "\n",
        "def load_model(label, metadata):\n",
        "    path = os.path.join(metadata['SAVE_PATH'], f\"{metadata['MODEL_NAME']}_{label}\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(path, local_files_only=True)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(path, local_files_only=True)\n",
        "    optimizer = transformers.Adafactor(params=model.parameters())\n",
        "    optimizer.load_state_dict(torch.load(os.path.join(path, 'optim.pt')))\n",
        "    return model, tokenizer, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkE5XCMBycZp"
      },
      "source": [
        "#Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hz89HD2ZxlmA"
      },
      "outputs": [],
      "source": [
        "def split_prediction(prediction):\n",
        "    main_split = prediction.split(';')\n",
        "    answer = main_split[0].split('=')[-1].strip()\n",
        "    if answer == '' and len(main_split) >= 3:\n",
        "        answer = main_split[1].split('=')[-1].strip()\n",
        "    proof = main_split[-1].split('=')[-1].strip()\n",
        "    return answer, proof\n",
        "\n",
        "def check_prediction(prediction, possible_targets):\n",
        "    answer, proof = split_prediction(prediction)\n",
        "    targets = [t.split('+') for t in possible_targets.split('|')]\n",
        "    return [answer, proof] in targets\n",
        "\n",
        "class ProofStageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, split, metadata, limit=None, offset=None, alt_format=False, check_length=False, legacy_random=False, legacy_bias=False):\n",
        "        self.max_source_length = metadata['MAX_SOURCE_LENGTH']\n",
        "        self.max_target_length = metadata['MAX_TARGET_LENGTH']\n",
        "        self.tokenizer = tokenizer\n",
        "        with open(os.path.join(metadata['DATASET_HOME'], split), 'rt') as data:\n",
        "            self.data = [json.loads(line) for line in data]\n",
        "        if limit is not None:\n",
        "            self.data = self.data[:limit]\n",
        "        if offset is not None:\n",
        "            self.data = self.data[offset:]\n",
        "        self.alt_format = alt_format\n",
        "        self.check_length = check_length\n",
        "        self.legacy_random = legacy_random\n",
        "        self.legacy_bias = legacy_bias\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        question = \"What is one singlehop inference?\"\n",
        "        if self.legacy_random:\n",
        "            pre_theory = [t['text'] for l in [self.data[index]['triples'].values(),self.data[index]['rules'].values()] for t in l]\n",
        "            random.shuffle(pre_theory)\n",
        "            alt_theory = [(k,v['text']) for l in [self.data[index]['triples'].items(),self.data[index]['rules'].items()] for k, v in l]\n",
        "            theory = []\n",
        "            mask = [False for _ in pre_theory]\n",
        "            for t1 in pre_theory:\n",
        "                flag = False\n",
        "                for i, t2 in enumerate(alt_theory):\n",
        "                    if t2[1] == t1 and mask[i] == False and flag == False:\n",
        "                        theory.append(t2)\n",
        "                        mask[i] = True\n",
        "                        flag = True\n",
        "        else:\n",
        "            theory = [(k,v['text']) for l in [self.data[index]['triples'].items(),self.data[index]['rules'].items()] for k, v in l]\n",
        "            random.shuffle(theory)\n",
        "        context = ' '.join(f\"sent{i+1}: {t[1]}\" for i, t in enumerate(theory))\n",
        "        targets = []\n",
        "        inferences = self.data[index]['allInferences']\n",
        "        for inf in inferences:\n",
        "            ans = inf['text']\n",
        "            proofs = inf['proofs'].split('OR')\n",
        "            for prf in proofs:\n",
        "                pre_tags = list(filter(None, [re.sub(r\"[->()\\[\\]]\", '', s) for s in re.split(' ', prf)]))\n",
        "                tags = [f\"sent{i+1}\" for x in pre_tags for i, t in enumerate(theory) if x == t[0]]\n",
        "                if self.legacy_bias:\n",
        "                    for i in range(0, len(theory)-1):\n",
        "                        for j in range(i+1, len(theory)):\n",
        "                            if theory[i][1] == theory[j][1] and f\"sent{j+1}\" in tags:\n",
        "                                tags[tags.index(f\"sent{j+1}\")] = f\"sent{i+1}\"\n",
        "                assert len(tags) >= 2 and len(tags) <= 3\n",
        "                if self.alt_format:\n",
        "                    format = f\"# {tags[0]} {tags[1]}\" if len(tags) == 2 else f\"# {tags[0]} & {tags[1]} {tags[2]}\"\n",
        "                else:\n",
        "                    format = f\"# {tags[1]} {tags[0]}\" if len(tags) == 2 else f\"# {tags[2]} & {tags[0]} {tags[1]}\"\n",
        "                targets.append((ans,format))\n",
        "        if len(targets) == 0:\n",
        "            targets.append((\"Nothing.\",\"None\"))\n",
        "        if self.legacy_random:\n",
        "            if len(inferences) > 0:\n",
        "                answer_choice = random.randint(0, len(inferences)-1)\n",
        "                proof_choice = random.randint(0, len(inferences[answer_choice]['proofs'].split('OR'))-1)\n",
        "                offset = sum(1 for i in range(answer_choice) for prf in inferences[i]['proofs'].split('OR'))\n",
        "                choice = offset + proof_choice\n",
        "            else:\n",
        "                choice = 0\n",
        "        else:\n",
        "            choice = random.randint(0, len(targets)-1)\n",
        "        answer = targets[choice][0]\n",
        "        proof = targets[choice][1]\n",
        "        source_text = f\"$answer$ ; $proof$ ; $question$ = {question} ; $context$ = {context}\"\n",
        "        target_text = f\"$answer$ = {answer} ; $proof$ = {proof}\"\n",
        "        source = self.tokenizer(\n",
        "            source_text,\n",
        "            padding = 'max_length',\n",
        "            max_length = self.max_source_length,\n",
        "            pad_to_max_length = True,\n",
        "            truncation = True,\n",
        "            return_tensors = 'pt',\n",
        "        )\n",
        "        target = self.tokenizer(\n",
        "            target_text,\n",
        "            padding = 'max_length',\n",
        "            max_length = self.max_target_length,\n",
        "            pad_to_max_length = True,\n",
        "            truncation = True,\n",
        "            return_tensors = 'pt',\n",
        "        )\n",
        "        if self.check_length:\n",
        "            assert source['input_ids'].squeeze().to(dtype=torch.long)[-1] == 0\n",
        "            assert target['input_ids'].squeeze().to(dtype=torch.long)[-1] == 0\n",
        "        return {\n",
        "            'source_ids': source['input_ids'].squeeze().to(dtype=torch.long),\n",
        "            'source_mask': source['attention_mask'].squeeze().to(dtype=torch.long),\n",
        "            'target_ids': target['input_ids'].squeeze().to(dtype=torch.long),\n",
        "            'target_ids_y': target['attention_mask'].squeeze().to(dtype=torch.long),\n",
        "            'possible_targets': '|'.join(['+'.join(x) for x in targets]),\n",
        "            'source_text': source_text,\n",
        "        }\n",
        "\n",
        "class ProofIterativeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, split, metadata, limit=None, offset=None):\n",
        "        self.max_source_length = metadata['MAX_SOURCE_LENGTH']\n",
        "        self.max_target_length = metadata['MAX_TARGET_LENGTH']\n",
        "        self.tokenizer = tokenizer\n",
        "        with open(os.path.join(metadata['DATASET_HOME'], split), 'rt') as data:\n",
        "            self.data = [json.loads(line) for line in data]\n",
        "        if limit is not None:\n",
        "            self.data = self.data[:limit]\n",
        "        if offset is not None:\n",
        "            self.data = self.data[offset:]\n",
        "        self.items = list()\n",
        "        for d in self.data:\n",
        "            theory = [v['text'] for l in [d['triples'].items(),d['rules'].items()] for _, v in l]\n",
        "            questions = [(v['question'][:-1]+'?',str(v['answer'])) for v in d['questions'].values()]\n",
        "            for q in questions:\n",
        "                random.shuffle(theory)\n",
        "                t = ' '.join(f'sent{i+1}: {x}' for i, x in enumerate(theory))\n",
        "                self.items.append((t, q[0], q[1]))\n",
        "        if limit is not None:\n",
        "            self.items = self.items[:limit]\n",
        "        if offset is not None:\n",
        "            self.items = self.items[offset:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        context, question, answer = self.items[index]\n",
        "        return {\n",
        "            'context': context,\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-Ny4sSc5gBJ"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lSWcvdIX7VOQ"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, tokenizer, optimizer, loader, metadata):\n",
        "    model.to(metadata['DEVICE'])\n",
        "    model.train()\n",
        "    cumulative_loss = 0\n",
        "    for i, data in enumerate(loader):\n",
        "        y = data['target_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "        y_ids = y[:,:-1].contiguous()\n",
        "        lm_labels = y[:,1:].clone().detach()\n",
        "        lm_labels[y[:,1:]==tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "        mask = data['source_mask'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "        loss = outputs.loss\n",
        "        cumulative_loss += loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return cumulative_loss / len(loader)\n",
        "\n",
        "def test_epoch(model, tokenizer, loader, metadata, nb=4, rp=2.5, lp=0.5, es=False):\n",
        "    model.to(metadata['DEVICE'])\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    possible_targets = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            y = data['target_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "            ids = data['source_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "            mask = data['source_mask'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask,\n",
        "                max_length = metadata['MAX_TARGET_LENGTH'],\n",
        "                num_beams = nb,\n",
        "                repetition_penalty = rp,\n",
        "                length_penalty = lp,\n",
        "                early_stopping = es,\n",
        "            )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            predictions.extend(preds)\n",
        "            possible_targets.extend(data['possible_targets'])\n",
        "    results = [check_prediction(p, t) for p, t in zip(predictions, possible_targets)]\n",
        "    accuracy = sum([x for x in results]) / len(results)\n",
        "    return accuracy\n",
        "\n",
        "def training_loop(model, tokenizer, optimizer, training_loader, validation_loader, testing_loader, metadata, save_best=True, save_last=True, base_accuracy=-1.0):\n",
        "    best_accuracy = base_accuracy\n",
        "    for epoch in range(metadata['EPOCHS']):\n",
        "        loss = train_epoch(model, tokenizer, optimizer, training_loader, metadata)\n",
        "        my_print(\"TRAIN\", f\"epoch: {epoch+1} | loss: {loss:.5f}\", mirror=metadata['MIRROR_OUTPUT'])\n",
        "        accuracy = test_epoch(model, tokenizer, validation_loader, metadata)\n",
        "        my_print(\"VAL  \", f\"epoch: {epoch+1} | accuracy: {accuracy:.3f}\", mirror=metadata['MIRROR_OUTPUT'])\n",
        "        if save_last:\n",
        "            save_model(model, tokenizer, optimizer, 'last', metadata)\n",
        "            my_print(\"SAVE \", \"model: last\", mirror=metadata['MIRROR_OUTPUT'])\n",
        "        if accuracy > best_accuracy:\n",
        "            if save_best:\n",
        "                save_model(model, tokenizer, optimizer, 'best', metadata)\n",
        "                my_print(\"SAVE \", \"model: best\", mirror=metadata['MIRROR_OUTPUT'])\n",
        "            best_accuracy = accuracy\n",
        "    last_model = model\n",
        "    accuracy = test_epoch(last_model, tokenizer, testing_loader, metadata)\n",
        "    my_print(\"TEST \", f\"model: last | accuracy: {accuracy:.3f}\", mirror=metadata['MIRROR_OUTPUT'])\n",
        "    if save_best:\n",
        "        best_model, _, _ = load_model('best', metadata)\n",
        "        accuracy = test_epoch(best_model, tokenizer, testing_loader, metadata)\n",
        "        my_print(\"TEST \", f\"model: best | accuracy: {accuracy:.3f}\", mirror=metadata['MIRROR_OUTPUT'])\n",
        "    else:\n",
        "        best_model = None\n",
        "    return last_model, best_model\n",
        "\n",
        "def single_inference(model, tokenizer, context, metadata, nb=4, rp=2.5, lp=0.5, es=False):\n",
        "    model.to(metadata['DEVICE'])\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        source_text = f\"$answer$ ; $proof$ ; $question$ = What is one singlehop inference? ; $context$ = {context}\"\n",
        "        source = tokenizer(\n",
        "            source_text,\n",
        "            padding = 'max_length',\n",
        "            max_length = metadata['MAX_SOURCE_LENGTH'],\n",
        "            pad_to_max_length = True,\n",
        "            truncation = True,\n",
        "            return_tensors = 'pt',\n",
        "        )\n",
        "        source_ids = source['input_ids'].to(dtype=torch.long)\n",
        "        source_mask = source['attention_mask'].to(dtype=torch.long)\n",
        "        ids = source_ids.to(metadata['DEVICE'], dtype=torch.long)\n",
        "        mask = source_mask.to(metadata['DEVICE'], dtype=torch.long)\n",
        "        generated_ids = model.generate(\n",
        "            input_ids = ids,\n",
        "            attention_mask = mask,\n",
        "            max_length = metadata['MAX_TARGET_LENGTH'],\n",
        "            num_beams = nb,\n",
        "            repetition_penalty = rp,\n",
        "            length_penalty = lp,\n",
        "            early_stopping = es,\n",
        "            num_return_sequences = nb,\n",
        "            output_scores = True,\n",
        "            return_dict_in_generate = True,\n",
        "        )\n",
        "        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids.sequences]\n",
        "        scores = torch.softmax(generated_ids.sequences_scores, dim=-1).tolist()\n",
        "    return [r for r in zip(preds, scores)]\n",
        "\n",
        "def check_ids_length(tokenizer, metadata):\n",
        "    training_set = ProofStageDataset(tokenizer, metadata['TRAIN_DATASET'], metadata, check_length=True)\n",
        "    training_loader = DataLoader(training_set, batch_size=metadata['BATCH_SIZE'], shuffle=False)\n",
        "    validation_set = ProofStageDataset(tokenizer, metadata['VAL_DATASET'], metadata, check_length=True)\n",
        "    validation_loader = DataLoader(validation_set, batch_size=metadata['BATCH_SIZE'], shuffle=False)\n",
        "    testing_set = ProofStageDataset(tokenizer, metadata['TEST_DATASET'], metadata, check_length=True)\n",
        "    testing_loader = DataLoader(testing_set, batch_size=metadata['BATCH_SIZE'], shuffle=False)\n",
        "    for loader in [training_loader, validation_loader, testing_loader]:\n",
        "        for data in loader:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference"
      ],
      "metadata": {
        "id": "KIDWU6SuYEvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentence(string, lemmifier, pattern='[^a-zA-Z0-9àèéìòùÀÈÉÌÒÙ ]'):\n",
        "    stripped_sentence = re.sub(pattern, '', string).lower()\n",
        "    tokenized_sentence = [t.lemma_ for t in lemmifier(stripped_sentence)]\n",
        "    return tokenized_sentence\n",
        "\n",
        "def search_answer(question, theory, spacy_module='en_core_web_md'):\n",
        "    lemmifier = spacy.load(spacy_module)\n",
        "    answer = 'Unknown'\n",
        "    proof = 'None'\n",
        "    tokenized_question = tokenize_sentence(question, lemmifier)\n",
        "    opposite_questions = list()\n",
        "    if 'not' in tokenized_question:\n",
        "        not_index = tokenized_question.index('not')\n",
        "        opposite_question_1 = tokenized_question.copy()\n",
        "        opposite_question_1.remove('not')\n",
        "        opposite_questions.append(opposite_question_1)\n",
        "        if not_index > 0 and tokenized_question[not_index-1] == 'do':\n",
        "            opposite_question_2 = opposite_question_1.copy()\n",
        "            opposite_question_2.pop(not_index-1)\n",
        "            opposite_questions.append(opposite_question_2)\n",
        "    tokenized_theory = [tokenize_sentence(t[1], lemmifier) for t in theory]\n",
        "    for i, t in enumerate(tokenized_theory):\n",
        "        if t == tokenized_question:\n",
        "            answer = 'True'\n",
        "            proof = theory[i][0]\n",
        "        elif t in opposite_questions:\n",
        "            answer = 'False'\n",
        "            proof = theory[i][0]\n",
        "        elif 'not' in t:\n",
        "            not_index = t.index('not')\n",
        "            opposite_t = t.copy()\n",
        "            opposite_t.remove('not')\n",
        "            if opposite_t == tokenized_question:\n",
        "                answer = 'False'\n",
        "                proof = theory[i][0]\n",
        "            elif not_index > 0 and opposite_t[not_index-1] == 'do':\n",
        "                opposite_t.pop(not_index-1)\n",
        "                if opposite_t == tokenized_question:\n",
        "                    answer = 'False'\n",
        "                    proof = theory[i][0]\n",
        "    return answer, proof\n",
        "\n",
        "def single_iterative_inference(model, tokenizer, metadata, file_name):\n",
        "    with open(file_name, 'r') as input_file:\n",
        "        lines = input_file.read().splitlines()\n",
        "    question = None\n",
        "    theory = []\n",
        "    input_data = ''\n",
        "    n = 1\n",
        "    for l in lines:\n",
        "        if l != '' and l[0] != '#':\n",
        "            if question is None:\n",
        "                question = l\n",
        "            else:\n",
        "                theory.append((f\"sent{n}\", l, \"None\"))\n",
        "                input_data += f\" sent{n}: {l}\"\n",
        "                n += 1\n",
        "    fact = None\n",
        "    while fact != 'Nothing.':\n",
        "        results = single_inference(model, tokenizer, input_data, metadata)\n",
        "        decoded_output = results[0][0]\n",
        "        fact, proof = split_prediction(decoded_output)\n",
        "        if fact != 'Nothing.':\n",
        "            input_data += f\" sent{n}: {fact}\"\n",
        "            theory.append((f\"sent{n}\", fact, proof))\n",
        "            n += 1\n",
        "    answer, proof = search_answer(question, theory)\n",
        "    return theory, question, answer, proof\n",
        "\n",
        "def iterative_test(model, tokenizer, loader, metadata, nb=4, rp=2.5, lp=0.5, es=False, verbose=False):\n",
        "    model.to(metadata['DEVICE'])\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    with torch.no_grad():\n",
        "        for n, data in enumerate(loader):\n",
        "            if verbose and (n + 1) % 10 == 0:\n",
        "                my_print(\"TEST \", f\"n: {n+1}/{len(loader)}\")\n",
        "            context = data['context'][0]\n",
        "            question = data['question'][0]\n",
        "            answer = data['answer'][0]\n",
        "            theory = [(t.split(':')[0].strip(),t.split(':')[1].strip()+'.',\"None\") for t in context.split('.')[:-1]]\n",
        "            fact = None\n",
        "            while fact != 'Nothing.':\n",
        "                source_text = f\"$answer$ ; $proof$ ; $question$ = What is one singlehop inference? ; $context$ = {context}\"\n",
        "                source = tokenizer(\n",
        "                    source_text,\n",
        "                    padding = 'max_length',\n",
        "                    max_length = metadata['MAX_SOURCE_LENGTH'],\n",
        "                    pad_to_max_length = True,\n",
        "                    truncation = True,\n",
        "                    return_tensors = 'pt',\n",
        "                )\n",
        "                ids = source['input_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "                mask = source['attention_mask'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "                generated_ids = model.generate(\n",
        "                    input_ids = ids,\n",
        "                    attention_mask = mask,\n",
        "                    max_length = metadata['MAX_TARGET_LENGTH'],\n",
        "                    num_beams = nb,\n",
        "                    repetition_penalty = rp,\n",
        "                    length_penalty = lp,\n",
        "                    early_stopping = es,\n",
        "                )\n",
        "                preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "                fact, proof = split_prediction(preds[0])\n",
        "                if fact != 'Nothing.':\n",
        "                    if fact not in [theory[i][1] for i, _ in enumerate(theory)]:\n",
        "                        number = len(theory)+1\n",
        "                        theory.append((f'sent{number}',fact,proof))\n",
        "                        context += f' sent{number}: {fact}'\n",
        "                    else:\n",
        "                        fact = 'Nothing.'\n",
        "            prediction, proof = search_answer(question, theory)\n",
        "            predictions.append(prediction)\n",
        "            targets.append(answer)\n",
        "            if verbose and prediction != answer:\n",
        "                my_print(\"WRONG\", f\"question: {question} | prediction: {prediction} ({proof}) | answer: {answer}\")\n",
        "                for t in theory:\n",
        "                    my_print(\"     \", f\"{t[0]}: {t[1]} ({t[2]})\", timestamp=False)\n",
        "    results = [p == t for p, t in zip(predictions, targets)]\n",
        "    accuracy = sum([x for x in results]) / len(results)\n",
        "    return accuracy\n",
        "\n",
        "def multibeam_test(model, tokenizer, loader, metadata, nb=4, rp=2.5, lp=0.5, es=False):\n",
        "    model.to(metadata['DEVICE'])\n",
        "    model.eval()\n",
        "    results = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            y = data['target_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "            ids = data['source_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "            mask = data['source_mask'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                max_length=metadata['MAX_TARGET_LENGTH'],\n",
        "                num_beams=nb,\n",
        "                repetition_penalty=rp,\n",
        "                length_penalty=lp,\n",
        "                early_stopping=es,\n",
        "                num_return_sequences=nb,\n",
        "            )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            chunks = [preds[i:i+nb] for i in range(0, len(preds), nb)]\n",
        "            for c, t in zip(chunks, data['possible_targets']):\n",
        "                result = check_prediction(c[0], t)\n",
        "                i = 0\n",
        "                while result == False and i < len(c):\n",
        "                    result = check_prediction(c[i], t)\n",
        "                    i += 1\n",
        "                results.append(result)\n",
        "    accuracy = sum([x for x in results]) / len(results)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "MAAdc1IVYIp1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Guidance"
      ],
      "metadata": {
        "id": "M5gRitxhF6PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_nlp_sm = spacy.load('en_core_web_sm')\n",
        "tree_nlp_md = spacy.load('en_core_web_md')\n",
        "tree_nlp_lg = spacy.load('en_core_web_lg')\n",
        "\n",
        "def get_tags(sentence, filter):\n",
        "    if sentence is not None:\n",
        "        tags = {x.lemma_ for x in list(tree_nlp_sm(sentence))+list(tree_nlp_md(sentence))+list(tree_nlp_lg(sentence)) if x.pos_ in filter}\n",
        "    else:\n",
        "        tags = set()\n",
        "    return list(tags), len(tags)\n",
        "\n",
        "def unpack_beams(beams, source):\n",
        "    pre_theory = source.split('=')[-1].split('.')\n",
        "    theory = {t.split(':')[0].strip(): t.split(':')[1].strip() for t in pre_theory[:-1]}\n",
        "    pre_candidates = [b.split(';') for b in beams]\n",
        "    answers = [pc[0].split('=')[-1].strip()[:-1] for pc in pre_candidates]\n",
        "    for i, _ in enumerate(answers):\n",
        "        if answers[i] == '' and len(pre_candidates[i]) >= 3:\n",
        "            answers[i] = pre_candidates[i][1].split('=')[-1].strip()\n",
        "    pre_proofs = [pc[-1].split('=')[-1].strip() for pc in pre_candidates]\n",
        "    pre_proofs = [pp.split(' ') for pp in pre_proofs]\n",
        "    proofs = []\n",
        "    for pp in pre_proofs:\n",
        "        if len(pp) == 5:\n",
        "            proofs.append((pp[1],pp[3],pp[4]))\n",
        "        elif len(pp) == 3:\n",
        "            proofs.append((pp[1],pp[2]))\n",
        "        elif len(pp) == 1 and pp[0] == 'None':\n",
        "            proofs.append(pp[0])\n",
        "        else:\n",
        "            proofs.append('')\n",
        "    candidates = [c for c in zip(answers, proofs)]\n",
        "    return candidates, theory\n",
        "\n",
        "def compute_plausibility(rule, answer, fact1, fact2=None, filter=['ADJ', 'VERB']):\n",
        "    tags_r, size_r = get_tags(rule, filter)\n",
        "    tags_a, size_a = get_tags(answer, filter)\n",
        "    tags_f1, size_f1 = get_tags(fact1, filter)\n",
        "    tags_f2, size_f2 = get_tags(fact2, filter)\n",
        "    matches_a = sum([1 if t in tags_r else 0 for t in tags_a])\n",
        "    matches_f1 = sum([1 if t in tags_r else 0 for t in tags_f1])\n",
        "    matches_f2 = sum([1 if t in tags_r else 0 for t in tags_f2])\n",
        "    if matches_a > 0 and matches_f1 > 0:\n",
        "        if fact2 is None:\n",
        "            score = True\n",
        "        else:\n",
        "            score = matches_f2 > 0\n",
        "    else:\n",
        "        score = False\n",
        "    return score, matches_a, matches_f1, matches_f2\n",
        "\n",
        "def compute_matches(beams, source):\n",
        "    candidates, theory = unpack_beams(beams, source)\n",
        "    reasonable = list()\n",
        "    matches = list()\n",
        "    for i, c in enumerate(candidates):\n",
        "        if c[0] == 'Nothing' and c[1] == 'None':\n",
        "            reasonable.append('None')\n",
        "            matches.append([])\n",
        "        elif len(c[1]) == 2 and c[1][0] != c[1][1] and c[1][0] in theory.keys() and c[1][1] in theory.keys() and c[0] not in theory.values():\n",
        "            answer = c[0]\n",
        "            rule = theory[c[1][0]]\n",
        "            fact = theory[c[1][1]]\n",
        "            plausible, matches_a, matches_f1, _ = compute_plausibility(rule, answer, fact, None)\n",
        "            reasonable.append('Yes' if plausible else 'No')\n",
        "            matches.append([matches_a,matches_f1,-1])\n",
        "        elif len(c[1]) == 3 and c[1][0] != c[1][1] and c[1][0] != c[1][2] and c[1][1] != c[1][2] and c[1][0] in theory.keys() and c[1][1] in theory.keys() and c[1][2] in theory.keys() and c[0] not in theory.values():\n",
        "            answer = c[0]\n",
        "            rule = theory[c[1][0]]\n",
        "            fact1 = theory[c[1][1]]\n",
        "            fact2 = theory[c[1][2]]\n",
        "            plausible, matches_a, matches_f1, matches_f2 = compute_plausibility(rule, answer, fact1, fact2)\n",
        "            reasonable.append('Yes' if plausible else 'No')\n",
        "            matches.append([matches_a,matches_f1,matches_f2])\n",
        "        else:\n",
        "            reasonable.append('Invalid')\n",
        "            matches.append([])\n",
        "    return matches, reasonable\n",
        "\n",
        "class GuidanceTree:\n",
        "\n",
        "    def __init__(self, model, tokenizer, metadata, load=None, verbose=False):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.metadata = metadata\n",
        "        if load is not None:\n",
        "            self.tree = joblib.load(load)\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def save(self, save='tree.joblib'):\n",
        "        joblib.dump(self.tree, save)\n",
        "\n",
        "    def learn(self, loader, nb=4, cross_val=True):\n",
        "        train_X = list()\n",
        "        train_y = list()\n",
        "        val_X = list()\n",
        "        val_y = list()\n",
        "        self.model.to(self.metadata['DEVICE'])\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for n, data in enumerate(loader):\n",
        "                if self.verbose and (n + 1) % 10 == 0:\n",
        "                    my_print(\"LEARN\", f\"n: {n+1}/{len(loader)} | samples: {len(train_y)+len(val_y)}\")\n",
        "                y = data['target_ids'].to(self.metadata['DEVICE'], dtype=torch.long)\n",
        "                ids = data['source_ids'].to(self.metadata['DEVICE'], dtype=torch.long)\n",
        "                mask = data['source_mask'].to(self.metadata['DEVICE'], dtype=torch.long)\n",
        "                generated_ids = self.model.generate(\n",
        "                    input_ids = ids,\n",
        "                    attention_mask = mask,\n",
        "                    max_length = self.metadata['MAX_TARGET_LENGTH'],\n",
        "                    num_beams = nb,\n",
        "                    repetition_penalty = 2.5,\n",
        "                    length_penalty = 0.5,\n",
        "                    early_stopping = False,\n",
        "                    num_return_sequences = nb,\n",
        "                    output_scores = True,\n",
        "                    return_dict_in_generate = True,\n",
        "                )\n",
        "                preds = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids.sequences]\n",
        "                beams = [preds[i:i+nb] for i in range(0, len(preds), nb)]\n",
        "                scores = [generated_ids.sequences_scores[i:i+nb] for i in range(0, len(generated_ids.sequences_scores), nb)]\n",
        "                confidences = [torch.softmax(s, dim=-1).tolist() for s in scores]\n",
        "                for b, c, t, s in zip(beams, confidences, data['possible_targets'], data['source_text']):\n",
        "                    matches, reasonable = compute_matches(b, s)\n",
        "                    i = 0\n",
        "                    found = -1\n",
        "                    sample = False\n",
        "                    while i < len(reasonable) and found == -1:\n",
        "                        if reasonable[i] == 'Yes':\n",
        "                            found = i\n",
        "                        elif reasonable[i] == 'None':\n",
        "                            found = i\n",
        "                            if i + 1 < len(reasonable) and reasonable[i+1] == 'Yes':\n",
        "                                sample = True\n",
        "                        i += 1\n",
        "                    if found == -1:\n",
        "                        found = 0\n",
        "                    if sample:\n",
        "                        data = list()\n",
        "                        data.append(c[found])\n",
        "                        data.append(c[found+1])\n",
        "                        data.append(c[found] - c[found+1])\n",
        "                        data += matches[found+1]\n",
        "                        answer = 1 if check_prediction(b[found+1], t) else 0\n",
        "                        if cross_val or len(train_y) % 2 == 0:\n",
        "                            train_X.append(data)\n",
        "                            train_y.append(answer)\n",
        "                        else:\n",
        "                            val_X.append(data)\n",
        "                            val_y.append(answer)\n",
        "        avg_scores = list()\n",
        "        for md in range(1, 15+1):\n",
        "            self.tree = tree.DecisionTreeClassifier(criterion='gini', max_depth=md)\n",
        "            if cross_val:\n",
        "                scores = model_selection.cross_val_score(self.tree, train_X, train_y, scoring='accuracy', cv=5)\n",
        "                avg_scores.append(np.mean(scores))\n",
        "            else:\n",
        "                self.tree.fit(train_X, train_y)\n",
        "                pred_y = self.tree.predict(val_X)\n",
        "                score = metrics.accuracy_score(val_y, pred_y)\n",
        "                avg_scores.append(score)\n",
        "        md = np.argmax(avg_scores)+1\n",
        "        if self.verbose:\n",
        "            my_print(\"TUNE \", f\"md: {md} | avg_scores: {avg_scores}\")\n",
        "        self.tree = tree.DecisionTreeClassifier(criterion='gini', max_depth=md)\n",
        "        self.tree.fit(train_X, train_y)\n",
        "\n",
        "    def infer(self, confidences, matches, index):\n",
        "        X = list()\n",
        "        data = [confidences[index],confidences[index+1],confidences[index]-confidences[index+1]]\n",
        "        data += matches[index+1]\n",
        "        X.append(data)\n",
        "        y = self.tree.predict(X)\n",
        "        return index if y[0] == 0 else index + 1\n",
        "\n",
        "    def draw(self):\n",
        "        plt.figure(figsize=(20,20))\n",
        "        tree.plot_tree(\n",
        "            self.tree,\n",
        "            fontsize = 10,\n",
        "            filled = True,\n",
        "            rounded = True,\n",
        "            proportion = True,\n",
        "            feature_names = ['conf_stop','conf_goon', 'conf_diff', 'match_answ','match_fac1','match_fac2'],\n",
        "            class_names = ['STOP','GOON'],\n",
        "        )\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "7RyeKAT53Lf6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def guided_test(model, tokenizer, pilot, loader, metadata, nb=4, rp=2.5, lp=0.5, es=False, verbose=False):\n",
        "    model.to(metadata['DEVICE'])\n",
        "    model.eval()\n",
        "    results = []\n",
        "    with torch.no_grad():\n",
        "        for n, data in enumerate(loader):\n",
        "            if verbose and (n + 1) % 10 == 0:\n",
        "                my_print(\"TEST \", f\"n: {n+1}/{len(loader)}\")\n",
        "            y = data['target_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "            ids = data['source_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "            mask = data['source_mask'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask,\n",
        "                max_length = metadata['MAX_TARGET_LENGTH'],\n",
        "                num_beams = nb,\n",
        "                repetition_penalty = rp,\n",
        "                length_penalty = lp,\n",
        "                early_stopping = es,\n",
        "                num_return_sequences = nb,\n",
        "                output_scores = True,\n",
        "                return_dict_in_generate = True,\n",
        "            )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids.sequences]\n",
        "            beams = [preds[i:i+nb] for i in range(0, len(preds), nb)]\n",
        "            scores = [generated_ids.sequences_scores[i:i+nb] for i in range(0, len(generated_ids.sequences_scores), nb)]\n",
        "            confidences = [torch.softmax(s, dim=-1).tolist() for s in scores]\n",
        "            for b, c, t, s in zip(beams, confidences, data['possible_targets'], data['source_text']):\n",
        "                matches, reasonable = compute_matches(b, s)\n",
        "                i = 0\n",
        "                prediction = None\n",
        "                while i < len(b) and prediction is None:\n",
        "                    if reasonable[i] == 'Yes':\n",
        "                        prediction = b[i]\n",
        "                    elif reasonable[i] == 'None':\n",
        "                        if i + 1 < len(reasonable) and reasonable[i+1] == 'Yes':\n",
        "                            prediction = b[pilot.infer(c, matches, i)]\n",
        "                        else:\n",
        "                            prediction = b[i]\n",
        "                    i += 1\n",
        "                if prediction is None:\n",
        "                    prediction = b[0]\n",
        "                result = check_prediction(prediction, t)\n",
        "                results.append(result)\n",
        "                if verbose and result == False:\n",
        "                    my_print(\"WRONG\", f\"beam: {b}\")\n",
        "                    my_print(\"     \", f\"conf: {c}\", timestamp=False)\n",
        "                    my_print(\"     \", f\"mtch: {matches} | reas: {reasonable}\", timestamp=False)\n",
        "                    my_print(\"     \", f\"pred: {prediction}\", timestamp=False)\n",
        "                    my_print(\"     \", f\"targ: {t}\", timestamp=False)\n",
        "    accuracy = sum([x for x in results]) / len(results)\n",
        "    return accuracy\n",
        "\n",
        "def iterative_guided_test(model, tokenizer, pilot, loader, metadata, nb=4, rp=2.5, lp=0.5, es=False, verbose=False):\n",
        "    model.to(metadata['DEVICE'])\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    with torch.no_grad():\n",
        "        for n, data in enumerate(loader):\n",
        "            if verbose and (n + 1) % 10 == 0:\n",
        "                my_print(\"TEST \", f\"n: {n+1}/{len(loader)}\")\n",
        "            context = data['context'][0]\n",
        "            question = data['question'][0]\n",
        "            answer = data['answer'][0]\n",
        "            theory = [(t.split(':')[0].strip(),t.split(':')[1].strip()+'.',\"None\") for t in context.split('.')[:-1]]\n",
        "            fact = None\n",
        "            while fact != 'Nothing.':\n",
        "                source_text = f\"$answer$ ; $proof$ ; $question$ = What is one singlehop inference? ; $context$ = {context}\"\n",
        "                source = tokenizer(\n",
        "                    source_text,\n",
        "                    padding = 'max_length',\n",
        "                    max_length = metadata['MAX_SOURCE_LENGTH'],\n",
        "                    pad_to_max_length = True,\n",
        "                    truncation = True,\n",
        "                    return_tensors = 'pt',\n",
        "                )\n",
        "                ids = source['input_ids'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "                mask = source['attention_mask'].to(metadata['DEVICE'], dtype=torch.long)\n",
        "                generated_ids = model.generate(\n",
        "                    input_ids = ids,\n",
        "                    attention_mask = mask,\n",
        "                    max_length = metadata['MAX_TARGET_LENGTH'],\n",
        "                    num_beams = nb,\n",
        "                    repetition_penalty = rp,\n",
        "                    length_penalty = lp,\n",
        "                    early_stopping = es,\n",
        "                num_return_sequences = nb,\n",
        "                output_scores = True,\n",
        "                return_dict_in_generate = True,\n",
        "                )\n",
        "                preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids.sequences]\n",
        "                beams = [preds[i:i+nb] for i in range(0, len(preds), nb)]\n",
        "                scores = [generated_ids.sequences_scores[i:i+nb] for i in range(0, len(generated_ids.sequences_scores), nb)]\n",
        "                confidences = [torch.softmax(s, dim=-1).tolist() for s in scores]\n",
        "                s = source_text\n",
        "                for b, c in zip(beams, confidences):\n",
        "                    matches, reasonable = compute_matches(b, s)\n",
        "                    i = 0\n",
        "                    prediction = None\n",
        "                    while i < len(b) and prediction is None:\n",
        "                        if reasonable[i] == 'Yes':\n",
        "                            prediction = b[i]\n",
        "                        elif reasonable[i] == 'None':\n",
        "                            if i + 1 < len(reasonable) and reasonable[i+1] == 'Yes':\n",
        "                                prediction = b[pilot.infer(c, matches, i)]\n",
        "                            else:\n",
        "                                prediction = b[i]\n",
        "                        i += 1\n",
        "                    if prediction is None:\n",
        "                        prediction = b[0]\n",
        "                fact, proof = split_prediction(prediction)\n",
        "                if fact != 'Nothing.':\n",
        "                    if fact not in [theory[i][1] for i, _ in enumerate(theory)]:\n",
        "                        number = len(theory)+1\n",
        "                        theory.append((f'sent{number}',fact,proof))\n",
        "                        context += f' sent{number}: {fact}'\n",
        "                    else:\n",
        "                        fact = 'Nothing.'\n",
        "            prediction, proof = search_answer(question, theory)\n",
        "            predictions.append(prediction)\n",
        "            targets.append(answer)\n",
        "            if verbose and prediction != answer:\n",
        "                my_print(\"WRONG\", f\"question: {question} | prediction: {prediction} ({proof}) | answer: {answer}\")\n",
        "                for t in theory:\n",
        "                    my_print(\"     \", f\"{t[0]}: {t[1]} ({t[2]})\", timestamp=False)\n",
        "                my_print(\"     \", f\"beam: {b} | conf: {c} | mtch: {matches} | reas: {reasonable}\", timestamp=False)\n",
        "    results = [p == t for p, t in zip(predictions, targets)]\n",
        "    accuracy = sum([x for x in results]) / len(results)\n",
        "    return accuracy\n",
        "\n",
        "def single_guided_inference(model, tokenizer, pilot, context, metadata, nb=4, rp=2.5, lp=0.5, es=False):\n",
        "    model.to(metadata['DEVICE'])\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        source_text = f\"$answer$ ; $proof$ ; $question$ = What is one singlehop inference? ; $context$ = {context}\"\n",
        "        source = tokenizer(\n",
        "            source_text,\n",
        "            padding = 'max_length',\n",
        "            max_length = metadata['MAX_SOURCE_LENGTH'],\n",
        "            pad_to_max_length = True,\n",
        "            truncation = True,\n",
        "            return_tensors = 'pt',\n",
        "        )\n",
        "        source_ids = source['input_ids'].to(dtype=torch.long)\n",
        "        source_mask = source['attention_mask'].to(dtype=torch.long)\n",
        "        ids = source_ids.to(metadata['DEVICE'], dtype=torch.long)\n",
        "        mask = source_mask.to(metadata['DEVICE'], dtype=torch.long)\n",
        "        generated_ids = model.generate(\n",
        "            input_ids = ids,\n",
        "            attention_mask = mask,\n",
        "            max_length = metadata['MAX_TARGET_LENGTH'],\n",
        "            num_beams = nb,\n",
        "            repetition_penalty = rp,\n",
        "            length_penalty = lp,\n",
        "            early_stopping = es,\n",
        "            num_return_sequences = nb,\n",
        "            output_scores = True,\n",
        "            return_dict_in_generate = True,\n",
        "        )\n",
        "        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids.sequences]\n",
        "        scores = torch.softmax(generated_ids.sequences_scores, dim=-1).tolist()\n",
        "        matches, reasonable = compute_matches(preds, source_text)\n",
        "        i = 0\n",
        "        prediction = None\n",
        "        while i < len(preds) and prediction is None:\n",
        "            if reasonable[i] == 'Yes':\n",
        "                prediction = preds[i]\n",
        "            elif reasonable[i] == 'None':\n",
        "                if i + 1 < len(reasonable) and reasonable[i+1] == 'Yes':\n",
        "                    prediction = preds[pilot.infer(scores, matches, i)]\n",
        "                else:\n",
        "                    prediction = preds[i]\n",
        "            i += 1\n",
        "        if prediction is None:\n",
        "            prediction = preds[0]\n",
        "    return prediction\n",
        "\n",
        "def single_guided_iterative_inference(model, tokenizer, pilot, metadata, file_name):\n",
        "    with open(file_name, 'r') as input_file:\n",
        "        lines = input_file.read().splitlines()\n",
        "    question = None\n",
        "    theory = []\n",
        "    input_data = ''\n",
        "    n = 1\n",
        "    for l in lines:\n",
        "        if l != '' and l[0] != '#':\n",
        "            if question is None:\n",
        "                question = l\n",
        "            else:\n",
        "                theory.append((f\"sent{n}\", l, \"None\"))\n",
        "                input_data += f\" sent{n}: {l}\"\n",
        "                n += 1\n",
        "    fact = None\n",
        "    while fact != 'Nothing.':\n",
        "        decoded_output = single_guided_inference(model, tokenizer, pilot, input_data, metadata)\n",
        "        fact, proof = split_prediction(decoded_output)\n",
        "        if fact != 'Nothing.':\n",
        "            if fact not in [theory[i][1] for i, _ in enumerate(theory)]:\n",
        "                input_data += f\" sent{n}: {fact}\"\n",
        "                theory.append((f\"sent{n}\", fact, proof))\n",
        "                n += 1\n",
        "            else:\n",
        "                fact = 'Nothing.'\n",
        "    answer, proof = search_answer(question, theory)\n",
        "    return theory, question, answer, proof"
      ],
      "metadata": {
        "id": "BXFOGMD0F7_L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzKZkyx6yfNT"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    metadata = {\n",
        "        'MODEL_NAME': 't5-small',\n",
        "        'SAVE_PATH': '/content/drive/MyDrive/ai_cloud_playground/',\n",
        "        'DATASET_HOME': './data/proofwriter-dataset-V2020.12.3/OWA/depth-3/',\n",
        "        'TRAIN_DATASET': 'meta-stage-train.jsonl',\n",
        "        'VAL_DATASET': 'meta-stage-dev.jsonl',\n",
        "        'TEST_DATASET': 'meta-stage-test.jsonl',\n",
        "        'BATCH_SIZE': 8,\n",
        "        'EPOCHS': 10,\n",
        "        'MAX_SOURCE_LENGTH': 640,\n",
        "        'MAX_TARGET_LENGTH': 64,\n",
        "        'DEVICE': set_device(gpu=True),\n",
        "        'SEED': set_seed(2147),\n",
        "        'MIRROR_OUTPUT': None,\n",
        "    }\n",
        "\n",
        "    # model = T5ForConditionalGeneration.from_pretrained(metadata['MODEL_NAME'])\n",
        "    # tokenizer = T5Tokenizer.from_pretrained(metadata['MODEL_NAME'], model_max_length=metadata['MAX_SOURCE_LENGTH'])\n",
        "    # model = T5ForConditionalGeneration.from_pretrained('rr_owa_d3plus_infstage_ma1_mixture_large_hf', local_files_only=True)\n",
        "    # tokenizer = T5Tokenizer.from_pretrained('t5-large', model_max_length=512)\n",
        "    # optimizer = transformers.Adafactor(params=model.parameters())\n",
        "    model, tokenizer, optimizer = load_model('best', metadata)\n",
        "    pilot = GuidanceTree(model, tokenizer, metadata, load=os.path.join(metadata['SAVE_PATH'], 't5-small_tree_validation.joblib'), verbose=True)\n",
        "\n",
        "    training_set = ProofStageDataset(tokenizer, metadata['TRAIN_DATASET'], metadata, limit=None)\n",
        "    training_loader = DataLoader(training_set, batch_size=metadata['BATCH_SIZE'], shuffle=True)\n",
        "    validation_set = ProofStageDataset(tokenizer, metadata['VAL_DATASET'], metadata, limit=None)\n",
        "    validation_loader = DataLoader(validation_set, batch_size=metadata['BATCH_SIZE'], shuffle=False)\n",
        "    testing_set = ProofStageDataset(tokenizer, metadata['TEST_DATASET'], metadata, limit=None)\n",
        "    testing_loader = DataLoader(testing_set, batch_size=metadata['BATCH_SIZE'], shuffle=False)\n",
        "    iter_test_set = ProofIterativeDataset(tokenizer, 'meta-test.jsonl', metadata, limit=3600)\n",
        "    iter_test_loader = DataLoader(iter_test_set, shuffle=True)\n",
        "\n",
        "    my_print(\"START\", mirror=metadata['MIRROR_OUTPUT'], reset=True)\n",
        "\n",
        "    # _, model = training_loop(model, tokenizer, optimizer, training_loader, validation_loader, testing_loader, metadata)\n",
        "    \n",
        "    # pilot.learn(dev_loader, cross_val=False)\n",
        "    # pilot.save(os.path.join(metadata['SAVE_PATH'], 'new_tree.joblib'))\n",
        "    # pilot.draw()\n",
        "\n",
        "    # accuracy = test_epoch(model, tokenizer, testing_loader, metadata)\n",
        "    # accuracy = multibeam_test(model, tokenizer, testing_loader, metadata, nb=4)\n",
        "    # accuracy = iterative_test(model, tokenizer, iter_test_loader, metadata)\n",
        "    # accuracy = guided_test(model, tokenizer, pilot, testing_loader, metadata, verbose=True)\n",
        "    accuracy = iterative_guided_test(model, tokenizer, pilot, iter_test_loader, metadata, verbose=True)\n",
        "    my_print(\"TEST \", f\"accuracy: {accuracy:.3f}\", mirror=metadata['MIRROR_OUTPUT'])\n",
        "\n",
        "    # input = 'sent1: Joe is green. sent2: Purple things are evil. sent3: Green people are funny.'\n",
        "    # result = single_inference(model, tokenizer, input, metadata)\n",
        "    # result = single_guided_inference(model, tokenizer, pilot, input, metadata)\n",
        "    # for p, s in result: print(f\"{p} {s*100:.1f}%\")\n",
        "\n",
        "    # file = os.path.join(metadata['SAVE_PATH'], 'theories', 'th_fantasy.txt')\n",
        "    # theory, question, answer, proof = single_iterative_inference(model, tokenizer, metadata, file)\n",
        "    # theory, question, answer, proof = single_guided_iterative_inference(model, tokenizer, pilot, metadata, file)\n",
        "    # for t in theory: print(f\"{t[0]}: {t[1]} ({t[2]})\")\n",
        "    # print(f\"{question} {answer} ({proof})\")\n",
        "\n",
        "    my_print(\"DONE \", mirror=metadata['MIRROR_OUTPUT'])"
      ],
      "metadata": {
        "id": "Nck5jRIcJD8L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear_cache()"
      ],
      "metadata": {
        "id": "a-dtM7Vyf0Ul"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"sent1: The cat is black. sent2: The dog is not big. sent3: Black cats are scary. sent4: Big dogs are scary.\"\n",
        "nb = 4\n",
        "model = T5ForConditionalGeneration.from_pretrained('rr_owa_d3plus_infstage_ma1_mixture_large_hf')\n",
        "model.to('cuda')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large', model_max_length=640)\n",
        "source = tokenizer(\n",
        "    f\"$answer$ ; $proof$ ; $question$ = What is one singlehop inference? ; $context$ = {context}\",\n",
        "    padding = 'max_length',\n",
        "    max_length = 640,\n",
        "    pad_to_max_length = True,\n",
        "    truncation = True,\n",
        "    return_tensors = 'pt' )\n",
        "source_ids = source['input_ids'].to(dtype=torch.long)\n",
        "source_mask = source['attention_mask'].to(dtype=torch.long)\n",
        "ids = source_ids.to('cuda', dtype=torch.long)\n",
        "mask = source_mask.to('cuda', dtype=torch.long)\n",
        "generated_ids = model.generate(\n",
        "    input_ids = ids,\n",
        "    attention_mask = mask,\n",
        "    max_length = 64,\n",
        "    num_beams = nb,\n",
        "    repetition_penalty = 2.5,\n",
        "    length_penalty = 0.5,\n",
        "    early_stopping = False,\n",
        "    num_return_sequences = nb,\n",
        "    output_scores = True,\n",
        "    return_dict_in_generate = True )\n",
        "predictions = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids.sequences]\n",
        "scores = torch.softmax(generated_ids.sequences_scores, dim=-1 ).tolist()\n",
        "for p, s in zip(predictions, scores):\n",
        "    print(f\"{p} ; $confidence$ = {s*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "a99be1adcd5e43a48acb52e5f2a9a1fc",
            "11c69607f89743e58f56f42489538d45",
            "b298d71e80424034bcc61fb2b32b9ef3",
            "97209b2dd0e84d5399743ccab31e3446",
            "63f47bef8f8743118324467f94a71c87",
            "5511a468ff7d4442a90c1ba9c6051696",
            "bfabfde269ee44bf89025614ede0c953",
            "e8aca4580f884f46b4a4e2503be4b335",
            "8dfa675271754fe3ab6868a6c20f6778",
            "5497b082642242d49fa200f5ef6b176e",
            "8fa3574f50f046d5a4958d24b46073c6",
            "d67164404f7d4cc5b482635a4cdb486c",
            "3de527eb6a8044dfb6ea57af2fd29ca8",
            "6d5e2514178a4aff9afea311e6ca1fc2",
            "0504aa913026431280297f1870126960",
            "7e3f3a7d8bb143edb59f258960b7297b",
            "84ad191cc1974bbbb574d8651d4db1a6",
            "3853eee111174d47a46c05cf9e0e8a95",
            "c26fea9f500c46ef872a5cc72c2be6dd",
            "d10d737edc2943569070ed153755f082",
            "9d4403e31b29484b8ae930ba332a39bf",
            "e48a5310b490427183bf671d288b3562"
          ]
        },
        "id": "Xvnolvbnzja0",
        "outputId": "4609857d-0f03-438c-b8fa-5351101568e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a99be1adcd5e43a48acb52e5f2a9a1fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d67164404f7d4cc5b482635a4cdb486c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$answer$ = The cat is scary. ; $proof$ = # sent3 sent1 ; $confidence$ = 50.5%\n",
            "$answer$ = The cat is scared. ; $proof$ = # sent3 sent1 ; $confidence$ = 20.3%\n",
            "$answer$ = The cat is quiet. ; $proof$ = # sent1 sent1 ; $confidence$ = 16.8%\n",
            "$answer$ = The cat is furry. ; $proof$ = # sent1 sent1 ; $confidence$ = 12.4%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a99be1adcd5e43a48acb52e5f2a9a1fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11c69607f89743e58f56f42489538d45",
              "IPY_MODEL_b298d71e80424034bcc61fb2b32b9ef3",
              "IPY_MODEL_97209b2dd0e84d5399743ccab31e3446"
            ],
            "layout": "IPY_MODEL_63f47bef8f8743118324467f94a71c87"
          }
        },
        "11c69607f89743e58f56f42489538d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5511a468ff7d4442a90c1ba9c6051696",
            "placeholder": "​",
            "style": "IPY_MODEL_bfabfde269ee44bf89025614ede0c953",
            "value": "Downloading: 100%"
          }
        },
        "b298d71e80424034bcc61fb2b32b9ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8aca4580f884f46b4a4e2503be4b335",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dfa675271754fe3ab6868a6c20f6778",
            "value": 791656
          }
        },
        "97209b2dd0e84d5399743ccab31e3446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5497b082642242d49fa200f5ef6b176e",
            "placeholder": "​",
            "style": "IPY_MODEL_8fa3574f50f046d5a4958d24b46073c6",
            "value": " 792k/792k [00:01&lt;00:00, 708kB/s]"
          }
        },
        "63f47bef8f8743118324467f94a71c87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5511a468ff7d4442a90c1ba9c6051696": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfabfde269ee44bf89025614ede0c953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8aca4580f884f46b4a4e2503be4b335": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dfa675271754fe3ab6868a6c20f6778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5497b082642242d49fa200f5ef6b176e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa3574f50f046d5a4958d24b46073c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d67164404f7d4cc5b482635a4cdb486c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3de527eb6a8044dfb6ea57af2fd29ca8",
              "IPY_MODEL_6d5e2514178a4aff9afea311e6ca1fc2",
              "IPY_MODEL_0504aa913026431280297f1870126960"
            ],
            "layout": "IPY_MODEL_7e3f3a7d8bb143edb59f258960b7297b"
          }
        },
        "3de527eb6a8044dfb6ea57af2fd29ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84ad191cc1974bbbb574d8651d4db1a6",
            "placeholder": "​",
            "style": "IPY_MODEL_3853eee111174d47a46c05cf9e0e8a95",
            "value": "Downloading: 100%"
          }
        },
        "6d5e2514178a4aff9afea311e6ca1fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c26fea9f500c46ef872a5cc72c2be6dd",
            "max": 1200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d10d737edc2943569070ed153755f082",
            "value": 1200
          }
        },
        "0504aa913026431280297f1870126960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d4403e31b29484b8ae930ba332a39bf",
            "placeholder": "​",
            "style": "IPY_MODEL_e48a5310b490427183bf671d288b3562",
            "value": " 1.20k/1.20k [00:00&lt;00:00, 39.2kB/s]"
          }
        },
        "7e3f3a7d8bb143edb59f258960b7297b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84ad191cc1974bbbb574d8651d4db1a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3853eee111174d47a46c05cf9e0e8a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c26fea9f500c46ef872a5cc72c2be6dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d10d737edc2943569070ed153755f082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d4403e31b29484b8ae930ba332a39bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48a5310b490427183bf671d288b3562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}